\chapter{Implementation}\label{chap:Algorithms}
\section{Baseline Team Implemenation} 
In this section, I will describe the addtional features described in the Techinical Specification draft and the implementation we made in the OpenUH compiler. 
Previously, we implemented support for Fortran coarrays in OpenUH in accordance with the Fortran 2008
specification~\cite{EachempatiCoarray2013}~\cite{EachempatiCAF2010}.  The OpenUH Coarray Fortran implementation is depicted in Figure~\ref{fig:teams-openuh}.  

\begin{figure}[!h]
\centering
\includegraphics[width=5in, height=4in]{figures/uh-coarrays-implementation-stack.eps}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{OpenUH Coarray Fortran team implementation}
\label{fig:teams-openuh}
\end{figure}

%\subsection{Runtime Design and Implementation for Teams} 

We added support into the Fortran front-end of OpenUH for parsing the
\texttt{form team},  \texttt{change team}, \texttt{end team} and \texttt{sync
team} constructs. We added the new type \texttt{team\_type} to the type system
of OpenUH and support for \texttt{get\_team} and \texttt{team\_number} intrinsics.
We also extended  the CAF intrinsics \texttt{this\_image},
\texttt{num\_images}, and \texttt{image\_index} for teams.
During the back-end compilation process in OpenUH, team-related constructs are
lowered to subroutine calls which constitute the \textit{libcaf} runtime
library interface. In the runtime, we added a \texttt{team\_type} data structure
for storing image-specific identification information, such as the mapping
from a new index to the process identifier in the lower communication layer.
The runtime also provides support for the team-related intrinsics
\texttt{get\_team} and \texttt{team\_number}.


Before team support was added into our implementation, coarray allocation was
globally symmetric across all images, with each coarray allocated at the same
offset within a managed symmetric heap.

With teams, however, this global symmetry is no longer necessary. According to the
draft of the technical specification, symmetric data objects have the
following features, which simplify the memory management for teams. First,
whenever two images are in the same team, they have the same memory
layout. Second, an image can only change to the initial team or teams formed
within  the current team. Third, when exiting a given team, all coarrays
allocated within this team should be deallocated automatically. And fourth, if
an image needs to refer to a coarray of another image located in a sibling
team, the coarray should be allocated in their common ancestor team.

Team variables are opaque, first-class objects which may be used to query
information for a specified team or change to a specified team. In our
implementation, a team variable refers to an associated team data structure,
depicted in Table~\ref{team-ds}. During the formation of a team, the values
for the fields of this data structure are computed and populated, including
(1) the list of images that are on the same node, (2) the number of images
within the same node, and (3) fields used to facilitate execution of
collective operations.  This information is used many times in the runtime by
different parallel algorithms (for example, collectives and barriers as
described in Section~\ref{sec:collectives}).
\begin{table}[!h]
%\begin{minipage}{1\columnwidth}%{\columnwidth}
%\centering

\caption{Team data structure}
\label{team-ds}
%\centering
%\hspace*{0.5in}

\begin{tabular}{|c|L{4.2in}|}
\hline
\textbf{Field} & \textbf{Description}  \\ 
\hline

 team\_num          & a team number or id, assigned during \texttt{form team} statement\\ \hline
 this\_image        & image index for current image in team \\ \hline
 num\_images        & number of images in team \\ \hline
intranode\_set      & ordered list of image indices in same compute node\\\hline
leader\_set         & ordered list of image indices of node leaders in team\\\hline
 leaders\_count     & number of node leaders in team\\\hline
 image\_index\_map  & maps image index to image index in inital team\\ \hline 
 sibling\_maps      & image index mapping for each sibling team created by same \texttt{form team} statement \\\hline
 bar\_parity                  & parity variable for dissemination barrier\\\hline 
bar\_sense                   & sense variable for dissemination barrier\\\hline
intranode\_bar\_flags        & direct shared pointers to intra-node barrier partners' flags\\\hline
bar\_rounds\_info            & partner information for inter-node in dissemination barrier rounds \\\hline
coll\_sync\_flags            & bcast, reduce, and allreduce sync flag\\     \hline
allreduce\_bufid             & selects between two allreduce buffers\\\hline
 reduce\_bufid                & selects between two reduce buffers\\\hline
 bcast\_bufid                 & selects between two bcast buffers \\\hline
 allocations     & a list of symmetric memory slots allocated for this team\\\hline 
 parent  		 & pointer to parent team structure\\\hline
\end{tabular}


\end{table}

\subsection{Memory Management}\label{sec:mm}

\begin{figure}[!h]
\centering
\begin{subfigure}{\textwidth}
	\centering
	\includegraphics[width=.55\linewidth]{figures/memory_step1_nt}
	\caption{Step 1: Allocate Coarray in initial team}
	\label{fig:mm_step1}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\centering
	\includegraphics[width=.55\linewidth]{figures/memory_step2_nt}
	\caption{Step 2: Form new team A and allocate Coarray on image 1, 2}
	\label{fig:mm_step2}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\centering
	\includegraphics[width=.55\linewidth]{figures/memory_step3_nt}
	\caption{Step 3: Form new team B and allocate Coarray on image 1}
	\label{fig:mm_step3}
\end{subfigure}
\begin{subfigure}{\textwidth}
	\centering
	\includegraphics[width=.55\linewidth]{figures/memory_step4_nt}
	\caption{Step 4: End team, exitting from team B}
	\label{fig:mm_step4}
\end{subfigure}
\caption{Evolving state of managed heap during team-relative
symmetric allocations}
\label{fig:memory-steps}
\end{figure}


%the asymmetric memory allocation ...
%the memory allocator logic using teams ...
%Edited by Gracia

Before incorporating support for teams, we implemented the managed heap as
follows. At the beginning of the program, the images collectively allocate a
pinned and registered memory segment which may be used for remote memory
accesses.  Static data which is allocated for the entire lifetime of the
program is placed in a reserved space at the top of this segment. The rest of
the segment is treated as a managed heap. Allocatable coarrays are
symmetrically and synchronously allocated on all images from the top of this
heap. We also allow non-symmetric allocations from the bottom of the heap.
This serves a few different purposes. We can allocate temporary communication
buffers from the bottom and avoid the cost of pinning and registering it.
Additionally, even though Fortran 2008 requires all coarrays to be symmetric
across all images, the coarrays may indirectly point to non-symmetric data.  This is
achieved by declaring the coarray to be of a derived data type with a pointer
or allocatable component, for which the target data may be allocated
independently of other images. This allocated data may then be remotely
referenced using the coarray.

In order to support coarray allocation with respect to teams, we considered a
few different approaches. The first approach was to reserve a
fixed-size memory container for each team, within which any coarray
allocations may be made. This could be achieved, for instance, through the use
of \textit{mspaces} in dlmalloc~\cite{dlmalloc}. However, such an approach
would require foreknowledge of how much space is required for each team, and in
general we expected a considerable waste of allocated space using this
approach.
The second approach which we settled on is to instead reserve a fixed size
heap for all teams except the initial team, which we call the \textit{teams
heap}. This turns out to be sufficient, since a coarray may only be allocated
for a non-initial team when that team is active and none of its descendant
teams have currently allocated coarrays. The initial team is an exception,
since at any time an image may execute a \texttt{change team} statement to
change back to the initial team, and hence a separate heap for the initial team is
still maintained.

\begin{figure}[H]
    \begin{minipage}{0.4\columnwidth}
     \lstset{language=Fortran,basicstyle=\tt\small,
     morekeywords={team, change}
     }
    \begin{lstlisting}
type(TEAM_TYPE) :: I,A,B
integer :: id
integer,allocatable :: d1(:)[:], &
                       d2(:)[:], &
                       d3(:)[:]

I = get_team()
allocate(d1(10)[*])
id = (this_image()-1)/2+1
form team(id, A)
change team(A)
  allocate(d2(10)[*])
  if(team_number() .eq. 1) then
    form team(this_image(), B)
    change team(B)
      allocate(d3(10)[*])
    end team !exit team B
 end if
end team

\end{lstlisting}
\end{minipage}
\caption{Code depicting allocation of coarrays inside teams}
\label{fig:demo-code}
\end{figure}




Figure~\ref{fig:memory-steps} illustrates how our managed heap
evolves over the course of the example program shown in Figure~\ref{fig:demo-code}.
When a coarray is allocated while executing a \texttt{change~team} block, corresponding
allocations should occur on all other images in the \textit{current} team,
rather than for all images.  Upon exiting the \texttt{change~team} block, any
allocations that had occurred within it are implicitly freed if they were not
already freed by a \texttt{deallocate} statement. An image may only
\textit{change} to a team with the \texttt{change team} construct if it was
formed by its current team with a \texttt{form team} statement or if it is the
initial team. The latter scenario requires that the state of symmetric
allocations belonging to the initial team should not be affected by
allocations (not yet freed) belonging to a non-initial team. To support this,
we reserve a fixed section of memory from the top of our managed heap for
symmetric allocations by a non-initial team. 
We divide the list structure for memory allocations into two lists: one is
for symmetric allocations by any non-initial team, and the other is for
symmetric allocations by the initial team and all non-symmetric allocations.
When changing to a new team, the \textit{allocations} field in the team
structure will be set to the current position in the non-initial team
allocations list.

\subsection{Forming and Changing Teams}

%We implement \texttt{form team}, \texttt{change team} and \texftt{end team} in runtime. 

The \texttt{form team} statement forms multiple teams by subdividing the set
of images that are members of the current team. Each image in the current
team must call the statement, specifying the number of the new team it will
join and a team variable which it may use to refer to the newly formed team. A
third optional argument may be specified to request a particular image index
within the new team. It is otherwise implementation-defined how these image
indices are assigned to team members; in our implementation, image indices are
assigned to each image in the order of their indices in the current team.

%code skeleton
Forming a new team entails a coordinated exchange of information from every
image in the current team. Our implementation is currently as shown in figure~\ref{fig:demo-form-team}. All the
images collectively perform an \textit{allgather} operation to exchange the
specified team numbers and (if given) image indices. Once this step completes,
each image can determine the members (and their respective image indices) for
each team formed. Based on this, each image can fill in the relevant
information in the team data structure which it associates with the newly
formed team. If a requested image index was specified with the \texttt{form
team} statement, this can be directly assigned to the \textit{this\_image}
field. Otherwise, the new image index is determined by sorting the set of
image indices which specified the same team number. The
\textit{image\_index\_mapping} field is a pointer to an array which maps an
image's image index in the new team to its image index in the initial team.The \textit{siblings}
field, shown in Table~\ref{team-ds}, is a pointer to an array of image maps
for every other new team formed. This is useful because an image may also
access a coarray belonging to a different team, using an extension to the
normal image selector syntax (e.g., $a[i, team\_number=2]$).  


In table~\ref{team-ds} we included two components related to the supernode information. The leader set contains the image indices for images in the team serving as designated leaders
for their respective compute nodes. The intra-node set contains the
image indices for all images in the team that share the same compute node.
The leader set and intra-node set may be computed based on the runtime's
determination of the process-to-node layout for the job. 

 For barriers, we
distinguish flags to be used for synchronization within a node via shared
memory (available through \textit{intranode\_bar\_flags}),
versus flags used to synchronize between images on separate nodes (available
through \textit{bar\_rounds\_info}). These flags are also stored in the team
data structure associated with each team, shown in Table~\ref{team-ds}.
Pointers for accessing a partner image's
synchronization flag at each round of the barrier are also precomputed and
stored at this stage.

\begin{figure}[H]
    \begin{minipage}{0.35\columnwidth}
     \lstset{language=C,basicstyle=\tt\small,
     morekeywords={team, change}
     }
    \begin{lstlisting}
void _form_team_( int *team_id, team_type * new_team_p,
			int * new_index) {
	/* input check */
	new_team_p = allocate_memory(new_team_p);
	team_info = form_teaminfo(team_id, new_index);
	_alltoall_exchange(&(team_info), exchange_buf, current_team);
	_calc_subteam(new_team_p, exchange_buf, *team_id);
	_calc_codimension_mapping(new_team_p, exchange_buf);
	}

void _alltoall_exchange(team_info_t * team_info, 
			team_info_t * exchange_buf, 
			team_type *current_team) {
	/*perform alltoall exchange function via shared buffer */
	}

void _calc_subteam(team_type *new_team_p, 
		team_info_t * exchange_buf, 
		int team_id) {
	/*calculate the size of new sub-team, 
	** fill the information into team structure */
	}
void _calc_codimension_mapping(team_type *new_team_p, 
				team_info_t * exchange_buf) {
	/*form up the codimension_mapping table
	** based on the information in buffer */
}
\end{lstlisting}
\end{minipage}
\caption{Code skeleton of FORM\_TEAM}
\label{fig:demo-form-team}
\end{figure}


The \texttt{change team} statement is used to change the \textit{current team}
in which the encountering image is executing to a team referenced by a team
variable argument. When an image executes this statement in our
implementation, it will simply change an internal \textit{current\_team}
pointer to the address of the team structure referenced by the team variable.
Next, all images changing to the same team will synchronize via an implicit
team barrier (note that the program should generally ensure that
all or none of the images in a team reach the statement, though its possible
for an image to check for stopped or failed images during its execution). When
\texttt{end team} is encountered, the runtime will set the internal
\textit{current\_team} to point to the parent of the current team. If leaving
a team which has itself created child teams, then the team structures
allocated for each of those child teams may be freed, and the corresponding
team variable will be set to a \texttt{NIL} value to indicate that it is no
longer associated with a team. If the team had allocated coarrays out of the
symmetric heap, the associated slots describing these allocations (in the
\textit{allocations} field) are freed.  Finally, all images in the team it
returns to must synchronize via an implicit team barrier. Note that whenever
an image switches to a different team, through the \texttt{change team} or
\texttt{end team} statements, it will always synchronize will all images which
are members of that team. This ensures that an image will never be executing
in a team while other members are executing in a different team. We make use
of this fact in the synchronization-avoidance optimizations we implemented for
collectives.

\subsection{Synchronization and Collective Operations}\label{sec:collectives}

Take the team into consideration, we need to modify the synchronization and collective operations, including barrier, synchronization and reductions. We currently support \textit{reduce}, \textit{allreduce}, \textit{broadcast},
and \textit{allgather} collective operations. The \textit{reduce} and
\textit{allreduce} support handles both pre-defined reduction operations
(\texttt{sum}, \texttt{min}, and \texttt{max}) as well as user-defined
reduction operations. The \textit{allgather} support is used
exclusively to facilitate formation of teams, and was implemented using
Bruck's algorithm~\cite{bruck1997efficient} with 1-sided communication. The \textit{reduce},
\textit{allreduce}, and \textit{broadcast} implementations are used for the
corresponding intrinsic subroutines -- \texttt{co\_reduce}, \texttt{co\_sum},
\texttt{co\_min}, \texttt{co\_max}, and \texttt{co\_broadcast}. We implemented
the respective binomial tree algorithms for reduction and broadcast, and the
recursive doubling algorithm for \textit{allreduce}. These well-known
algorithms, which we implemented using 1-sided communication, are described in~\cite{thakur2005optimization}. Each of these algorithms complete in $\log P$
steps for P images in a team, where on each step pairs of images are
communicating -- either a write from one image to the other (\textit{reduce}
and \textit{broadcast}), or independent writes from both images to the other
image in the pair (\textit{allreduce}).

During the team formation step, we allocate various synchronization flags
to be used for team-based barriers and collective operations. These procedures are not shown in the skeleton code in figure~\ref{fig:demo-form-team}.

Synchronization flags are also allocated and reserved for supported collective
operations (specifically, \textit{allreduce}, \textit{reduce}, and
\textit{broadcast}) that may be executed by the new team. This is necessary since we implement
collectives using 1-sided communication which is decoupled from
synchronization. Since these collectives entail different communication
structures, in order to allow for their execution to partially overlap we
allocate a distinct set of synchronization flags for each type during team
formation. 

Our collectives implementation makes use of a \textit{collectives buffer
space} --  a fixed size, symmetric region which is reserved from the remote
access memory segment. By default, the size is 4 MiB per image, but this may be
adjusted through an environment variable. If this reserved buffer space is large
enough, it will be used to carry out any of the data transfers required during
the execution of a collective operation. Otherwise, all the images in the team
will synchronously allocate a symmetric region of memory from the heap to
serve as the buffer space for the execution of the operation. In order to
carry out a \textit{reduce}, \textit{broadcast}, or \textit{allreduce}
operation, each image will reserve from its buffer space a \textit{base
buffer}.  The base buffer is used to hold the result of each step in the
collective operation. Additionally, for \textit{reduce} and \textit{allreduce}
each image will reserve at least one work buffer. The work buffers are used to
hold data communicated by a partner on each step, which will be merged into
the base buffer using the specified reduction operation. Our binomial tree
reduction and broadcast algorithms assume that the root image will be image 1
in the team. Therefore, we incur the cost of an additional communication to
image 1 (for \textit{broadcast}) or from image 1 (for \textit{reduce}) when
this is not the case. In the event that these operations are operating on
large arrays which can not be accommodated within the buffer space available
(determined by the image heap size), we can break up these arrays into chunks,
and perform our algorithm on each of these chunks in sequence.

\section{Runtime Optimizations}

\subsection{Runtime Data Locality Optimization}
The latency for inter-node remote memory accesses is typically significantly
higher compared to intra-node memory accesses. Therefore, a reasonable
strategy for collective operations that exhibit a fixed communication
structure (which is the case our \textit{reduce}, \textit{allreduce}, and
\textit{broadcast} algorithms)
is to restructure the communication in such a way that minimizes that required
inter-node communication. This is especially important when dealing with
collective operations for a team of images, where member images may be
distributed and fixed across the nodes in a non-uniform manner. We therefore developed
2-level algorithms for these collective operations which exploit the fact that
the operations are presumed to be commutative and associative.  %, similar to the 2-level
%dissemination barrier algorithms we described earlier in the chapter.
Each compute node which has at least one image in the team has a designated
leader image, and all the leaders in the team have a unique \textit{leader
index}.

During the initialization stage, the GASNet communication layer provides us a function \texttt{gasnet\_getNodeInfo()} to query the information of each gasnet node, which is image in this case. The node info structure includes an component called supernode, indicating the id of physical node, comprised by multiple cores that share a physical memory. 

When performing the \textit{reduce} or \textit{allreduce} operation, there are three phases. In
the first phase, team members residing on the same compute node will reduce to
the leader image. In the second phase, the leaders, will carry out either the
\textit{reduce} or \textit{allreduce} operation among themselves. After the second phase, the
first leader has the final result for \textit{reduce} operation, and all leaders have
the final result for the \textit{allreduce} operation. In the third and final phase,
for the \textit{reduce} operation the first leader will write the final result to the
root image, if it is not itself the root image. For the final phase of the
\textit{allreduce} operation, each leader image will broadcast its result to other
images on its compute node. Depending on the particular topology of the
compute node, this intra-node broadcast may be implemented using a binomial
tree algorithm, or by simply having all the non-leaders on a node read the
final result from the leader with the requisite synchronizations.

For the \textit{broadcast} operation, the three phases are as follows. In the first
phase, the source image will first write its data to the first leader image in
the team. In the second phase, the leaders will collectively perform a
binomial tree broadcast. In the final phase, each leader will broadcast its
result to the non-leaders on its node.

We also enhanced our barrier implementation by taking advantage of node
locality information provided by the underlying runtime layer, described
in~\cite{cafteams-cluster}.  Within the runtime descriptor for a team there is
a list of image indices for images that reside on the same node and a list of
image indices for images in the team which are designated leaders of their
respective nodes. Using this structure, we implemented a \textit{2-level} team
barrier algorithm as follows. When an image encounters the barrier, it will
either notify the leader image of its node, or if it is itself a leader it
will wait on notifications from the non-leaders. Once the leader image has
collected notifications from all other images in the same node, it engages in
a dissemination barrier with other leader images in the team, as described
above. Finally, once the dissemination barrier completes the leaders will
notify its non-leaders that all images in the team have encountered the
barrier. Each non-leader image will atomically increment a shared counter
residing on the leader image within the node to signal that it reached the
barrier, and it will wait for a synchronization flag to be toggled by the
leader to signal that the rest of the images have also reached the barrier.

\subsection{Distributed member mapping list}
Consider the actual team structure in memory shown in figure~\ref{fig:teams-structure}, the local memory usage is proportional to the number of images in this team. Besides, if we take the \textit{sibling team} into consider and store the image mapping lists into local team structure, the memory usage will be proportional to the total number of images in envrionment. The program can benefit from this all-in-local manner when the number of images is small because the lookup cost in local memory is O(1). But it is an obvious factor that hurts the scalibility. 

One reasonable way to resolve it is to distribute the mapping information into different images and we need a mechanism to query each time when we need to send the put/get communication calls. The \textit{team} structure only need to store a few information within it.

\begin{figure}[H]
\centering
\includegraphics[scale=0.58]{figures/team_structure.eps}
\caption{OpenUH Coarray Fortran team structure in memory}
\label{fig:teams-structure}
\end{figure}

Here I proposed and implemented the distributed member mapping list in a simple manner. Each image contains the mapping information from its predecessor and successor. So in a global view, the images form a double-linked list. Each time when a image launch a communication call to image that is not its predecessor or successor, it will send an \textit{active message} to look through this list. 

We demostrate the mechanism of \textit{active message} in figure~\ref{fig:am-imgidx} and implementation snippet in figure~\ref{fig:demo-imgidx}. Without much prove we can see the delay of the index query will hurts communication performance severely. 
\begin{figure}[h]
    \begin{minipage}{0.35\columnwidth}
     \lstset{language=C,basicstyle=\tt\small,
     morekeywords={team, change}
     }
    \begin{lstlisting}
int get_imgidx(team_type * this_team, uint32_t image) {
	/*check input */
	int32_t idx = -1;
	if(idx=find_in_local(this_team) != -1) return idx;
	else {
		if(IN_LEFT_SIDE(image, this_team)) 
			gasnet_AMRequestMedium1(this->team->pre, 
				GASNET_HANDLER_IMGIDX_REQUEST,
				...);
		else {
			gasnet_AMRequestMedium1(this->team->suc, 
				GASNET_HANDLER_IMGIDX_REQUEST,
				...);
		}	
		GASNET_BLOCKUNTIL();
	}
	...
	return idx;
}
\end{lstlisting}
\end{minipage}
\caption{Code skeleton of image index query}
\label{fig:demo-imgidx}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.58]{figures/amdemo.eps}
\caption{Active Message to query the image index}
\label{fig:am-imgidx}
\end{figure}
To reduce the unnecessary network delay, we should try to store as much information as possible in the local memory so that the query function will return at the first step. In addtion to this query function, we also implemented a cache mechanism to store most recently queried result. 

As we know there are several cache mechanism with different emphasis property. Here we implemented a simple LRU cache using hash table and double-linked list. We consider the amortized complexity of lookup operation on a hash table is O(1). The lookup and maintainess cost of this LRU is in constant complexity. 

%During the \texttt{FORM\_TEAM} stage
